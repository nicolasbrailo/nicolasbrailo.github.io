<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">

<channel>
  <title>Nico Brailovsky's thought repository</title>
  <link>https://nicolasbrailo.github.io/</link>
  <description>I write things. A few may make sense.</description>
  <lastBuildDate>2026-02-22</lastBuildDate>
  <pubDate>2026-02-22</pubDate>
  <item>
  <title>Weekend project: Raspberry Pi CRT</title>
  <link>https://nicolasbrailo.github.io//blog/2026/0222_PiCRT.html</link>
  <pubDate>2026-02-22</pubDate>
  <author>Nico Brailovsky</author>
  <description><![CDATA[
<p>In what may be the most useless project I've done in a long time, I spent the weekend making an old CRT work with my Raspberry Pi.</p>
<p><img alt="" src="/blog_img/0222_PiCRT.jpg" /></p>
<p>I don't think there will be much use for this project. Ignoring that this is a CRT (720x576 black-and-white), the TV I picked up is pretty noisy. I don't miss the high pitched whine of a CRT (mine is 11 KHz, if you're wondering). Still it was fun to make this work, and I did learn a few things:</p>
<ul>
<li>The Raspberry Pi has an SDTV composite/RCA video output. It's shared with the audio output jack. The audio out supports pins with 4 connectors (TRRS connector), and you can get video in one of them.</li>
<li>There are, of course, multiple standards for TRRS. A Pi uses TRRS CTIA, in which each connector of the pin is (tip to cable) left audio, right audio, video and ground. Unfortunately, many vendors don't specify which standard you're getting. If you get the wrong one, it's not complicated to rejig the cable to be CTIA, just a few snips and some soldering.</li>
<li>A lot of articles online will tell you that adding <code>sdtv_mode</code> to /boot/firmware/config.txt is enough to enable video out. I found that's not the case, you'll need to specify also <code>sdtv_aspect</code>, <code>enable_tvout</code> and <code>dtoverlay=vc4-fkms-v3d</code> (this last one enables firmware control of video out. I didn't dig into why this is needed, and KMS doesn't work).</li>
<li>You will also need to pin the core frequency. Frequency scaling will affect video rendering.</li>
</ul>
<p>I put all these <a href="https://github.com/nicolasbrailo/picrt/blob/main/check_sdtv.sh">setup steps in a convenient script</a>, available as part of the app I'm using to show pictures. Now I need to think what I can do with this ridiculously large piece of ancient tech, which has less resolution than my watch.</p>
  ]]></description>
</item>

<item>
  <title>Weekend project: Tripmon</title>
  <link>https://nicolasbrailo.github.io//blog/2026/0208_tripmon.html</link>
  <pubDate>2026-02-08</pubDate>
  <author>Nico Brailovsky</author>
  <description><![CDATA[
<p><a href="https://github.com/nicolasbrailo/tripmon">Tripmon is a way to visualize my trips</a> (and daytrips).</p>
<p><img alt="" src="https://raw.githubusercontent.com/nicolasbrailo/tripmon/refs/heads/main/README_screenshot2.jpg" /></p>
<p>I have (had?) a lot of data in gmaps and no good way to visualize, or merge, with my extensive album collection. So I built a small service to categorize pictures and merge them with map traces. The service will also try to score the "best" pictures, for whatever definition of "best" a few ML models choose, and display just a few highlights for each part of the trip.</p>
<p>This project falls, for the most part, in <a href="md_blog/2026/0118_AI.md">type 3 of my AI categorizaion</a>: largely vibe-coded, and if it breaks I wouldn't know why. This is in contrast with other weekend projects; I also spent some time adding voice control to my home automation system. I "care" about the code in my home automation system, but I don't care much about the code of my Tripmon project. Adding new features to my home automation system takes 10x the time it takes to add new features to Tripmon, as I go through a very through review and refactor process. In my home automation service, when things break I know exactly why and how (it's fairly important for me to be able to turn my lights on or off). In Tripmon, if something doesn't work I just ask AI to iterate, until it more or less does what I want.</p>
<p><img alt="" src="https://raw.githubusercontent.com/nicolasbrailo/tripmon/refs/heads/main/README_screenshot1.jpg" /></p>
<p>From the readme, Tripmon will:</p>
<ul>
<li>Scan a directory for photo albums, and derive day-trips from album names.</li>
<li>Group day-trips into trips, then generate a "report" for each trip.</li>
<li>Merge each trip with GPS traces from G Maps.</li>
<li>Run an analysis on your albums, and select the best N pictures (for whatever definition of "best" the model that looks at pictures may have)</li>
</ul>
<p>With this information, Tripmon will generate a report for each trip and day-trip. The report will include</p>
<ul>
<li>A map overlay with the visited locations and the transport between each</li>
<li>A list of places visited, and the time spent in each place</li>
<li>A list of pictures to go with each place</li>
</ul>
  ]]></description>
</item>

<item>
  <title>Weekend project: (Mini) audio science talk</title>
  <link>https://nicolasbrailo.github.io//blog/2026/0203_miniaudio.html</link>
  <pubDate>2026-02-03</pubDate>
  <author>Nico Brailovsky</author>
  <description><![CDATA[
<p>Another project to file under "brilliant ideas": I spent the weekend (and a bit more, really) working on a <a href="https://nicolasbrailo.github.io/SlidewareEngineering/MiniAudio/">set of experiments to teach how audio works, for children</a>.</p>
<p>While <a href="https://nicolasbrailo.github.io/SlidewareEngineering">publishing my set of JS audio demos</a> a few weeks back (<a href="md_blog/2026/0119_JSAudioDemos.md">wonder if I need to credit AI, too</a>), I figured I should try to widen my audience beyond other software engineers. The material consists of simplified explanations of how audio transmission works, and how computers work with audio. Finding a balance between oversimplifying things and staying topic-relevant for children has been a challenge, but I'm quite happy with the final results. There are also plenty of experiments that, hopefully, will keep the session engaging.</p>
<p>While I doubt many 8 year olds are reading these notes, I'll be getting some feedback on this sessions soon. Wish me luck!</p>
  ]]></description>
</item>

<item>
  <title>Raspberry Pi Karaoke Machine</title>
  <link>https://nicolasbrailo.github.io//blog/2026/0124_karaoke.html</link>
  <pubDate>2026-01-24</pubDate>
  <author>Nico Brailovsky</author>
  <description><![CDATA[
<p>I had a brilliant idea to setup a karaoke machine for a party. Working with audio and computers means I always have a fresh supply of microphones, speakers and rpi's in diverse state of brokenness, so I figured it shouldn't be too hard to throw everything together and try to build a karaoke machine. It was easier than I expected, and it only took a couple of hours, so here's a guide to repeat the same process when I need it next year.</p>
<h2>BoM:<a name="bom"></a></h2>
<ul>
<li>Rpi 4+</li>
<li>A touchscreen</li>
<li>A [portable] speaker with aux input</li>
<li>Some USB microphones, ideally using an audio DIN connector</li>
</ul>
<p>A touchscreen will make the system portable without too much hassle. Also, prefer wired connections in the system: you could use bluetooth mics/speakers, and your life will be simpler by doing so, but each bluetooth hop will add quite a bit of latency, up to 200ms. May not seem like much, but 200ms is the equivalent of ~70 meters: imagine if you had to shout to someone 70 meters away?</p>
<p>Why DIN? USB cables have a length limit, and unless you have top of the line expensive USB cables you are likely limited to 1 meter, maybe 2 (and let's be honest, if you're assembling a karaoke machine out of spare parts, how likely is it that you have a lot of expensive USB cables lying around?) A DIN mic will have a much more permissive length limit, allowing you to go for 5 or even 10 meters with a cheap cable. This makes up for the range you lost by not using bluetooth. It makes the system more cumbersome, as you need to deal with long cables, but also a lot more resilient. And if you are reading this, you probably ENJOY cable management anyway.</p>
<p><img alt="" src="/blog_img/0630_HouseboardP0/2PP0.jpg" /></p>
<p>For this build, I'm <a href="md_blog/projects_texts/24homeboard.md">reusing the beautiful industrial design of my P00 Homeboard</a>: an RPI4 wirezipped to a touchscreen. I didn't use POE, however, as the power requirements of the USB mics + preamps go beyond what 802.3af can offer (less than 15W!).</p>
<h2>Software<a name="software"></a></h2>
<ol>
<li>Get your base rpi OS installed as usual</li>
<li>Install <a href="https://github.com/vicwomg/pikaraoke">PiKaraoke</a>. While you can go for a Docker container or a pipenv, I think it's easier to <code>pip3 install pikaraoke --break-system-packages</code> and make this a system (user) package. I'll just wipe the OS for my next project anyway.</li>
<li>PiKaraoke will need a js runtime, the page explains how to install one. Again, easier option is to make it a system install and just wipe the OS for the next project.</li>
<li>apt-get install qpwgraph: we will use this to create a mic/speaker loopback (ie the karaoke part of the system)</li>
</ol>
<h2>Runtime setup<a name="runtimesetup"></a></h2>
<p>The default rpi OS includes an on screen keyboard for touchscreens. It's cumbersome to use, but the setup is simple enough that it's just about doable. If you expect to use this as more than a temporary setup, you may want to automate the steps below to run on startup.</p>
<p>When starting the system, use qpwgraph to create a loop between your mic(s) and the speaker. This was a lot harder in the ALSA/pulseaudio days, but with Pipewire it's trivial. Be careful with the echo: place the speaker far enough from the mics to avoid creating a feedback loop. Maybe a future version of this <a href="https://nicolasbrailo.github.io/SlidewareEngineering/StopCopyingMe/">system will include an echo canceller</a>? Try it out to ensure the loopback works fine.</p>
<p>Run <code>./usr/local_bin/pikaraoke</code> (or wherever the install put the binary). This will start the service. From there, just set up the system with your phone using the QR code it displays.</p>
<h2>Latency<a name="latency"></a></h2>
<p>Keeping latency down is important for this build. Once you have it running, I recommend running a quick latency test. You will need a metronome (or any other thing that can produce periodic clicks, and lets you control the tempo). Get the metronome close to the mic, and put your ear close to the speaker with a volume low enough that the mic doesn't pick up echo.</p>
<p>With this setup, you should hear two clicks: once from the metronome, and once from the speaker, after having gone through the system. Adjust the tempo until you can hear a single click. When you do, it means that the loopback latency of the system equals the latency between clicks: the time it takes for sound to trouble from the mic, through the OS and back through the speaker, is the same as the time it takes the metronome to produce two clicks (plus some acoustic delay, which is below your ears measurement error for this setup anyway).</p>
<p>If your metronome is running at 120 bpm when the two clicks "merge", your system latency is around 500ms. My RPI+USB mics was around 300ish. High, but usable. For a next build, I should try to get this down to 100 or less.</p>
  ]]></description>
</item>

<item>
  <title>Slideware engineering: My audio demos</title>
  <link>https://nicolasbrailo.github.io//blog/2026/0119_JSAudioDemos.html</link>
  <pubDate>2026-01-19</pubDate>
  <author>Nico Brailovsky</author>
  <description><![CDATA[
<p>Writing <a href="md_blog/2026/0118_AI.md">a note on AI</a> made me think of a good example of "using AI to do a thing I wouldn't have done otherwise". This example falls within the third taxonomy I describe in the note: AI isn't just augmenting my code, but actively writing large chunks of it. The results are loosely based on my examples, but I don't actually understand large chunks of it.</p>
<p>I've been sitting on examples and training material on how to work with audio. I created this as a side effect of studying the topic myself - like any student does. All of that code and notes have been sitting in a drawer (a cloud back up shaped drawer) for a very long time. Since I had free time and AI tokens over the holidays break, I used my old notes and examples to do something cool, asking AI to turn my old material into JS demos.</p>
<p>For audio, JS demos can yield pretty impressive results. I am, for example, <a href="https://nicolasbrailo.github.io/SlidewareEngineering/AirToArrays/#/logHumanHearing">particularly happy with this demo</a>, showing how human hearing is logarithmic. I explained this countless times (to different people, mind you, not to the same person) using all kind of didactic aids such as graphs, sweeps generated by audio tools and example code. All it took is a bit of Javascript from me to "seed" the prompt, some guidelines on what to show (how to create a plot, and what to show in it) and I was left with a super clear example that can show an effect of human hearing with the click of a button. Next time I need to explain this topic, it should take me 10x less time.</p>
<p>These code examples, together with my notes and an old <a href="https://nicolasbrailo.github.io/SlidewareEngineering/00_js_slides_template/#/title">template based on Impress JS I've used for ages</a>, and my old studying material is now transformed into something resembling passable how-to-audio sessions, with cool interactive demos.</p>
<p>Check out <a href="https://nicolasbrailo.github.io/SlidewareEngineering/AirToArrays/">"Arrays to Air"</a> for a basic explanation of digital audio processing, including an abuse of WebAudio oscillators to create the worst iFFT the world has ever seen. Also check out <a href="https://nicolasbrailo.github.io/SlidewareEngineering/StopCopyingMe/">"Stop Copying Me"</a> for a more in-depth explanation of how echo cancellation works for telephony applications. There are some more in my <a href="https://nicolasbrailo.github.io/SlidewareEngineering/">SlidewareEngineering index</a>, which I hope to update as I release new ones.</p>
  ]]></description>
</item>

<item>
  <title>Dear AI overlords</title>
  <link>https://nicolasbrailo.github.io//blog/2026/0118_AI.html</link>
  <pubDate>2026-01-18</pubDate>
  <author>Nico Brailovsky</author>
  <description><![CDATA[
<p>It's 2026 and I haven't written about AI. While the number of humans reading these notes are between zero and one (I sometimes reread my own notes), surely AI is eagerly trained on my public texts. Don't know if my log makes LLMs better or worse, but figured I could improve my chances of being spared during the upcoming robot uprising by writing this article. Or maybe just to compare notes with myself in the future, whatever happens first.</p>
<ol>
<li>AI is like using a GPS navigation app: you still need to know where you want to go, and how you want to get there (bike, walk or drive?). You delegate things to an agent, and you will get worse at those. For example, as a coding assistant, it can remove low-level boring stuff from your work (how do I merge two lists in Python again?). The next time you need to perform the same task you are unlikely to remember how to do so, just how people are <a href="https://www.nature.com/articles/s41598-020-62877-0">less likely to learn how to get from A to B when using a navigation app</a>.</li>
<li>AI can be used as a super manual, an assistant to augment your code, or to write code.</li>
<li>The effect of having a super manual is obvious (such as helping you find papers you read a long time ago, like the one I used just now on effects of navigation apps on human spatial ability). This is undeniably useful, but that's just a better search engine.</li>
<li>Augmenting your code is a good way of speeding up your work, though not the 10x speedup claimed. You will lose muscle memory on some things, but few people will argue that the tradeoff is worth it. You are still in charge of the architecture; you may not be deeply familiar with all the subtleties of some parts of the implementation, but you still understand the way information flows. Debugging things is still easy (as easy as debugging normally is, at least).</li>
<li>When asking an agent to write code, your program is now the prompt. The code is an artifact much like assembly is an artifact of your c code. Unlike c code, your program isn't deterministic anymore. Like an assembly artifact, it's likely you don't understand it. You <em>can</em> build that understanding (for now?), though this will be as fun as trying to understand other people's code (and remember LLMs are the <em>average</em> of all programmers out there).</li>
</ol>
<p>These are random notes and observations. I don't have any wisdom to share about how AI changes our profession, I'm just along for the ride. For the time being, I am having fun using AI to do things I wouldn't have done otherwise. I recently built a <a href="https://github.com/nicolasbrailo/zmw/tree/main/zmw_cat_snack_dispenser">Cat feeder service</a> with Zigbee and Telegram integration. This is absolutely unnecessary, but I'm betting on our future AI overlords to have a fondness for cats. The training material makes me think AI will like cats more than humans. Can you blame it?</p>
<p><a href="https://github.com/nicolasbrailo/zmw/tree/main/zmw_cat_snack_dispenser"><img alt="" src="https://raw.githubusercontent.com/nicolasbrailo/zmw/main/zmw_cat_snack_dispenser/README_screenshot.png" /></a></p>
<p>Disclaimer: no AI has been used to write notes in this blog, this is still a manual efforrt and all of the mistakes here are carefully handcrafted by humans (a single human, actually).</p>
  ]]></description>
</item>

<item>
  <title>I like Makefiles</title>
  <link>https://nicolasbrailo.github.io//blog/2025/1207_ILikeMakefiles.html</link>
  <pubDate>2025-12-07</pubDate>
  <author>Nico Brailovsky</author>
  <description><![CDATA[
<p>Confession time: I like Makefiles!</p>
<p>With the baitclick out of the way: Makefiles, in 2025, can still be incredibly useful. Traditionally we think about Makefiles as a build system, however I realized it works much better as a list of notes. For my projects, I tend to use Makefiles as a documentation mechanism to remember things I did, and may need to repeat in the future. A few examples:</p>
<ol>
<li>
<p>I keep my list of deps in Makefiles: I tend to keep a <a href="https://github.com/nicolasbrailo/homeboard/blob/main/Makefile#L71">target called 'system_deps' or similar</a>, where I can see which apt-get's I ran to get a specific service up and running. This extends to other things that already have their own "history" in place, <a href="https://github.com/nicolasbrailo/zigbee2mqtt2web/blob/master/Makefile#L38">like pipfiles</a>, but I found less than reliable in the past: when moving between targets with different architectures, for example, I found dealing with pipfiles quite tedious. My trusty <code>make system_deps</code> may take longer and is less elegant, but has never failed me so far!</p>
</li>
<li>
<p>Testing is easier with Makefiles: Running test targets can make life a lot easier. Sure, I could remember that <code>wlr-randr --output HDMI-A-1 --off</code> will shutdown a display... if I did it every day. I can also read the manual, or even create a small script to "remember" it. But it's a lot neater to keep these <a href="https://github.com/nicolasbrailo/wl-display-toggle/blob/main/Makefile#L11">small, project-dependent, one-off commands</a> as a list in my Makefile. Then I only need to <code>cd</code> to a project, and <code>make &lt;tab&gt;&lt;tab&gt;</code> to remember how to test things.</p>
</li>
<li>
<p>Self-testing documentation: I keep <a href="https://github.com/nicolasbrailo/rpiz-xcompile/blob/main/Makefile#L1">targets that are the equivalent of a hello-world</a>, but quickly let me document how a complex system is meant to be used. Whenever I need to ramp-up a new project, or go back to a project after a few months, a Makefile can help me get up to speed in a few minutes.</p>
</li>
<li>
<p>Building things, write-only: Ok this one doesn't fall in the "documentation" category but unsurprisingly, <code>make</code> is actually <a href="https://github.com/nicolasbrailo/homeboard_ambience/blob/9ae0470935734603277ec0c181268ca5f4a4ea25/Makefile#L74">pretty useful at building things</a>. There may be better, more modern and certainly more maintainable options, however few are as simple as Makefiles. Yes, Makefiles code is horrible. For anything except the most trivial work, I consider them write-only code: you write it once, and no one can ever decipher how they work, ever again. Need to make a change to a Makefile? Better start from scratch, with a blank file. It will save you time.</p>
</li>
</ol>
<p>As long as you work within the constrains of the tool (keep it simple, or accept it's write-only code), Makefiles are still a wonderful tool 50 years after their invention.</p>
  ]]></description>
</item>

<item>
  <title>Homeboard: Versioning frames</title>
  <link>https://nicolasbrailo.github.io//blog/2025/0323_HomeboardFrames.html</link>
  <pubDate>2025-03-23</pubDate>
  <author>Nico Brailovsky</author>
  <description><![CDATA[
<p>Since I've been fixing plenty of bugs, figured I should also start versioning my frame mount designs.</p>
<p>The Ikea-frame version should look something like this:</p>
<p><a href="/blog_img/2025/0315_HomeboardNewFrameMount1.jpg"><img alt="" src="/blog_img/2025/0315_HomeboardNewFrameMount1.jpg" /></a></p>
<p>The design for this one lives here</p>
<p><a href="https://github.com/nicolasbrailo/homeboard/blob/main/mount_designs/MountForIkeaFrame.svg"><img alt="" src="/blog_img/2025/0323_IkeaFrame.jpg" /></a></p>
<p>You can download it an open it with Inkscape; remember to <a href="md_blog/2025/0209_HomeboardIndustrialDesign.md">switch to outline mode in Inkscape</a>, otherwise you're unlikely to see anything. The frames are designed for a laser engraver, and the cuts are about 1/100'th of a mm.</p>
<p>And the standalone vesion will hopefully look a bit less terrible than this, since this picture is from a few bug-revisions before:</p>
<p><a href="/blog_img/250216_Homeboard.jpg"><img alt="" src="/blog_img/250216_Homeboard.jpg" /></a></p>
<p>The design for the standalone version:</p>
<p><a href="https://github.com/nicolasbrailo/homeboard/blob/main/mount_designs/MountForStandaloneFrame.svg"><img alt="" src="/blog_img/2025/0323_StandaloneFrame.jpg" /></a></p>
  ]]></description>
</item>

<item>
  <title>Homeboard: A Hardware bug!</title>
  <link>https://nicolasbrailo.github.io//blog/2025/0316_HomeboardHardwareBug.html</link>
  <pubDate>2025-03-16</pubDate>
  <author>Nico Brailovsky</author>
  <description><![CDATA[
<p>I found my first hardware bug! Can you spot it? It's the big red circle:</p>
<p><a href="/blog_img/2025/0316_HomeboardHardwareBug1.jpg"><img alt="" src="/blog_img/2025/0316_HomeboardHardwareBug1.jpg" /></a></p>
<p>The mmwave sensor was mounted too close to either the screen, or the power source (something I thought was a brilliant idea yesterday). Turns out that mounting it so close has an affect on this sensor: when the display is on, it blocks the sensor (and reads it as no-presence). When the display is off, for some reason the sensor picks it up as someone being present. This is bad, because on presence I turn the display on, and on vacancy off. I guess my living room put on a light show for my cats last night.</p>
<p>I suspect I could fix this in the firmware of the sensor, but that's pointless because <a href="md_blog/2024/0615_LD2410SmmWaveSensor.md">I can't reverse engineer the sensor protocol anyway</a>. What's the next best fix?</p>
<p><a href="/blog_img/2025/0316_HomeboardHardwareBug2.jpg"><img alt="" src="/blog_img/2025/0316_HomeboardHardwareBug2.jpg" /></a></p>
<p>I moved the sensor out of the way, while I think of a better placement.</p>
  ]]></description>
</item>

<item>
  <title>Homeboard: eInk display</title>
  <link>https://nicolasbrailo.github.io//blog/2025/0315_HomeboardNewFrameMount.html</link>
  <pubDate>2025-03-15</pubDate>
  <author>Nico Brailovsky</author>
  <description><![CDATA[
<p>Homeboard gained a new form factor: slightly less crappy frame.</p>
<p><a href="/blog_img/2025/0315_HomeboardNewFrameMount1.jpg"><img alt="" src="/blog_img/2025/0315_HomeboardNewFrameMount1.jpg" /></a></p>
<p>I now keep two Homeboards, one in my office -mostly for hacking- and one to display pictures. The one in my office didn't have a <a href="md_blog/2025/0223_HomeboardEInkDisplay.md">good space for the eInk display</a> (spoiler alert: it still doesn't) making it awkward to see both the "real" display and the eink one. To fix this, I built a new mount based on a picture frame. This time all of the elements are mounted directly on the front frame (spoiler alert: this was a huge mistake), and I used transparent perspex material to cut it, so that all elements are visible (I do like this bit, the boards that make up Homeboard are quite pretty).</p>
<h2>Mechanics<a name="mechanics"></a></h2>
<p>The build uses an Ikea picture frame, but replaces the front plate with my laser-cut front.</p>
<ul>
<li>The Ikea frame is great for this, it's built to support a front plate of 3-6mm, fitting a perspex sheet ferpectly.</li>
<li>I'm happy with the display corner clips, too. You can see in the picture they hold the display, but are not too obtrusive (only partly due to the clips being transparent). Additionally, they are great to clip on small boards with no mount holes, like the radar sensor (top left in the picture).</li>
<li>The ribbon connection to the display is hell. The position is awkward, and I can't fit it with a short (2cm) cable. I used a long one (15cm) but it looks untidy.</li>
<li>Don't overtighten display screws! It's easy to put too much pressure and damage either the two perspex sheets, or the sandwiched display in the middle. I found for a 3mm perspex sheet with a laptop display, 10mm m2 screws loosely tightened (?) work best.</li>
<li>If you use my mechanical drawings, be careful: between <a href="md_blog/2025/0209_HomeboardIndustrialDesign.html">ID V1</a> and this one, there was bitrot in my svg, and the screws in the pi don't align anymore. Also, the display hole isi about 2mm too big for my panel, and I don't know why (my last cut it was 2mm to small!)</li>
</ul>
<p>The back of the frame:</p>
<p><a href="/blog_img/2025/0315_HomeboardNewFrameMount2.jpg"><img alt="" src="/blog_img/2025/0315_HomeboardNewFrameMount2.jpg" /></a></p>
<p>Some things I need to improve:</p>
<ul>
<li>Ribbon, long or short, placing is super hard. For V2 of this ID, I need to think of a better placement</li>
<li>In fact, mounting everything to the front panel was a big mistake. It means that mounting things is awkward, because I need to work with a big panel. Any wiring mistake means I need to unmount the board, fix, test, remount. It's much much MUCH easier if I mount all the boards to a single main perspex board, then mount that to the main frame.</li>
<li>Having a main board with alternative mount position should make it easier to make mounting the ribbon cable less terrible. I need to move the edp board 20mm to the right in this ID, but it's much easier if I don't need to carefully align this before I cut it.</li>
<li>The corner clips are awesome! I can even use to hold sensors without a screw hole. Here I mounted the mmwave sensor (with no mount screw holes) using one of the corner clips.</li>
<li>This doesn't work for the eInk display, unfortunately. I still need to figure out how to mount the eInk display without using tape.</li>
</ul>
  ]]></description>
</item>

</channel>
</rss>

<!DOCTYPE html>
<html>
<head>

<link rel="icon" href="/favicon.ico" type="image/x-icon"/>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
<title>Posts for 2026 January Nico Brailovsky's thought repository</title>

<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="/style.css">
</head>

<body>

<div id="siteheader">
  <h1>Nico Brailovsky's thought repository</h1>

<div class="nav tabs is-full">
  <a class="is-center" href="/blog/index.html">Blog</a>
  <a class="is-center"
     href="https://github.com/search?type=code&q=repo%3Anicolasbrailo%2Fnicolasbrailo.github.io%20"
     onclick="togglesearch(); return false">Site search</a>
  <a class="is-center" href="/blog/projects_texts">Projects & Texts</a>
  <a class="is-center" href="/blog/history.html">Archive</a>
  <a class="is-center" href="/blog/aboutme.html">About</a>
</div>

<form id="sitesearch" class="nav tabs is-full is-hidden">
  <input type="text" id="sitesearch_q"/>
  <button type="submit">Search</button>
</form>

</div>

<div id="content" class="language-clike">
<h2>Posts for 2026 January</h2>

<h2>Raspberry Pi Karaoke Machine<a name="raspberrypikaraokemachine"></a></h2>
<p>Post by Nico Brailovsky @ 2026-01-24 | <a href="/blog/2026/0124_karaoke.html">Permalink</a>  | <a href="https://github.com/nicolasbrailo/nicolasbrailo.github.io/issues/new?title=Comment@/blog/2026/0124_karaoke.html&amp;body=I%20have%20a%20comment!">Leave a comment</a></p>
<p>I had a brilliant idea to setup a karaoke machine for a party. Working with audio and computers means I always have a fresh supply of microphones, speakers and rpi's in diverse state of brokenness, so I figured it shouldn't be too hard to throw everything together and try to build a karaoke machine. It was easier than I expected, and it only took a couple of hours, so here's a guide to repeat the same process when I need it next year.</p>
<h2>BoM:<a name="bom"></a></h2>
<ul>
<li>Rpi 4+</li>
<li>A touchscreen</li>
<li>A [portable] speaker with aux input</li>
<li>Some USB microphones, ideally using an audio DIN connector</li>
</ul>
<p>A touchscreen will make the system portable without too much hassle. Also, prefer wired connections in the system: you could use bluetooth mics/speakers, and your life will be simpler by doing so, but each bluetooth hop will add quite a bit of latency, up to 200ms. May not seem like much, but 200ms is the equivalent of ~70 meters: imagine if you had to shout to someone 70 meters away?</p>
<p>Why DIN? USB cables have a length limit, and unless you have top of the line expensive USB cables you are likely limited to 1 meter, maybe 2 (and let's be honest, if you're assembling a karaoke machine out of spare parts, how likely is it that you have a lot of expensive USB cables lying around?) A DIN mic will have a much more permissive length limit, allowing you to go for 5 or even 10 meters with a cheap cable. This makes up for the range you lost by not using bluetooth. It makes the system more cumbersome, as you need to deal with long cables, but also a lot more resilient. And if you are reading this, you probably ENJOY cable management anyway.</p>
<p><img alt="" src="/blog_img/0630_HouseboardP0/2PP0.jpg" /></p>
<p>For this build, I'm <a href="/blog/projects_texts/24homeboard.html">reusing the beautiful industrial design of my P00 Homeboard</a>: an RPI4 wirezipped to a touchscreen. I didn't use POE, however, as the power requirements of the USB mics + preamps go beyond what 802.3af can offer (less than 15W!).</p>
<h2>Software<a name="software"></a></h2>
<ol>
<li>Get your base rpi OS installed as usual</li>
<li>Install <a href="https://github.com/vicwomg/pikaraoke">PiKaraoke</a>. While you can go for a Docker container or a pipenv, I think it's easier to <code>pip3 install pikaraoke --break-system-packages</code> and make this a system (user) package. I'll just wipe the OS for my next project anyway.</li>
<li>PiKaraoke will need a js runtime, the page explains how to install one. Again, easier option is to make it a system install and just wipe the OS for the next project.</li>
<li>apt-get install qpwgraph: we will use this to create a mic/speaker loopback (ie the karaoke part of the system)</li>
</ol>
<h2>Runtime setup<a name="runtimesetup"></a></h2>
<p>The default rpi OS includes an on screen keyboard for touchscreens. It's cumbersome to use, but the setup is simple enough that it's just about doable. If you expect to use this as more than a temporary setup, you may want to automate the steps below to run on startup.</p>
<p>When starting the system, use qpwgraph to create a loop between your mic(s) and the speaker. This was a lot harder in the ALSA/pulseaudio days, but with Pipewire it's trivial. Be careful with the echo: place the speaker far enough from the mics to avoid creating a feedback loop. Maybe a future version of this <a href="https://nicolasbrailo.github.io/SlidewareEngineering/StopCopyingMe/">system will include an echo canceller</a>? Try it out to ensure the loopback works fine.</p>
<p>Run <code>./usr/local_bin/pikaraoke</code> (or wherever the install put the binary). This will start the service. From there, just set up the system with your phone using the QR code it displays.</p>
<h2>Latency<a name="latency"></a></h2>
<p>Keeping latency down is important for this build. Once you have it running, I recommend running a quick latency test. You will need a metronome (or any other thing that can produce periodic clicks, and lets you control the tempo). Get the metronome close to the mic, and put your ear close to the speaker with a volume low enough that the mic doesn't pick up echo.</p>
<p>With this setup, you should hear two clicks: once from the metronome, and once from the speaker, after having gone through the system. Adjust the tempo until you can hear a single click. When you do, it means that the loopback latency of the system equals the latency between clicks: the time it takes for sound to trouble from the mic, through the OS and back through the speaker, is the same as the time it takes the metronome to produce two clicks (plus some acoustic delay, which is below your ears measurement error for this setup anyway).</p>
<p>If your metronome is running at 120 bpm when the two clicks "merge", your system latency is around 500ms. My RPI+USB mics was around 300ish. High, but usable. For a next build, I should try to get this down to 100 or less.</p>
<hr />
<h2>Slideware engineering: My audio demos<a name="slidewareengineeringmyaudiodemos"></a></h2>
<p>Post by Nico Brailovsky @ 2026-01-19 | <a href="/blog/2026/0119_JSAudioDemos.html">Permalink</a>  | <a href="https://github.com/nicolasbrailo/nicolasbrailo.github.io/issues/new?title=Comment@/blog/2026/0119_JSAudioDemos.html&amp;body=I%20have%20a%20comment!">Leave a comment</a></p>
<p>Writing <a href="/blog/2026/0118_AI.html">a note on AI</a> made me think of a good example of "using AI to do a thing I wouldn't have done otherwise". This example falls within the third taxonomy I describe in the note: AI isn't just augmenting my code, but actively writing large chunks of it. The results are loosely based on my examples, but I don't actually understand large chunks of it.</p>
<p>I've been sitting on examples and training material on how to work with audio. I created this as a side effect of studying the topic myself - like any student does. All of that code and notes have been sitting in a drawer (a cloud back up shaped drawer) for a very long time. Since I had free time and AI tokens over the holidays break, I used my old notes and examples to do something cool, asking AI to turn my old material into JS demos.</p>
<p>For audio, JS demos can yield pretty impressive results. I am, for example, <a href="https://nicolasbrailo.github.io/SlidewareEngineering/AirToArrays/#/logHumanHearing">particularly happy with this demo</a>, showing how human hearing is logarithmic. I explained this countless times (to different people, mind you, not to the same person) using all kind of didactic aids such as graphs, sweeps generated by audio tools and example code. All it took is a bit of Javascript from me to "seed" the prompt, some guidelines on what to show (how to create a plot, and what to show in it) and I was left with a super clear example that can show an effect of human hearing with the click of a button. Next time I need to explain this topic, it should take me 10x less time.</p>
<p>These code examples, together with my notes and an old <a href="https://nicolasbrailo.github.io/SlidewareEngineering/00_js_slides_template/#/title">template based on Impress JS I've used for ages</a>, and my old studying material is now transformed into something resembling passable how-to-audio sessions, with cool interactive demos.</p>
<p>Check out <a href="https://nicolasbrailo.github.io/SlidewareEngineering/AirToArrays/">"Arrays to Air"</a> for a basic explanation of digital audio processing, including an abuse of WebAudio oscillators to create the worst iFFT the world has ever seen. Also check out <a href="https://nicolasbrailo.github.io/SlidewareEngineering/StopCopyingMe/">"Stop Copying Me"</a> for a more in-depth explanation of how echo cancellation works for telephony applications. There are some more in my <a href="https://nicolasbrailo.github.io/SlidewareEngineering/">SlidewareEngineering index</a>, which I hope to update as I release new ones.</p>
<hr />
<h2>Dear AI overlords<a name="dearaioverlords"></a></h2>
<p>Post by Nico Brailovsky @ 2026-01-18 | <a href="/blog/2026/0118_AI.html">Permalink</a>  | <a href="https://github.com/nicolasbrailo/nicolasbrailo.github.io/issues/new?title=Comment@/blog/2026/0118_AI.html&amp;body=I%20have%20a%20comment!">Leave a comment</a></p>
<p>It's 2026 and I haven't written about AI. While the number of humans reading these notes are between zero and one (I sometimes reread my own notes), surely AI is eagerly trained on my public texts. Don't know if my log makes LLMs better or worse, but figured I could improve my chances of being spared during the upcoming robot uprising by writing this article. Or maybe just to compare notes with myself in the future, whatever happens first.</p>
<ol>
<li>AI is like using a GPS navigation app: you still need to know where you want to go, and how you want to get there (bike, walk or drive?). You delegate things to an agent, and you will get worse at those. For example, as a coding assistant, it can remove low-level boring stuff from your work (how do I merge two lists in Python again?). The next time you need to perform the same task you are unlikely to remember how to do so, just how people are <a href="https://www.nature.com/articles/s41598-020-62877-0">less likely to learn how to get from A to B when using a navigation app</a>.</li>
<li>AI can be used as a super manual, an assistant to augment your code, or to write code.</li>
<li>The effect of having a super manual is obvious (such as helping you find papers you read a long time ago, like the one I used just now on effects of navigation apps on human spatial ability). This is undeniably useful, but that's just a better search engine.</li>
<li>Augmenting your code is a good way of speeding up your work, though not the 10x speedup claimed. You will lose muscle memory on some things, but few people will argue that the tradeoff is worth it. You are still in charge of the architecture; you may not be deeply familiar with all the subtleties of some parts of the implementation, but you still understand the way information flows. Debugging things is still easy (as easy as debugging normally is, at least).</li>
<li>When asking an agent to write code, your program is now the prompt. The code is an artifact much like assembly is an artifact of your c code. Unlike c code, your program isn't deterministic anymore. Like an assembly artifact, it's likely you don't understand it. You <em>can</em> build that understanding (for now?), though this will be as fun as trying to understand other people's code (and remember LLMs are the <em>average</em> of all programmers out there).</li>
</ol>
<p>These are random notes and observations. I don't have any wisdom to share about how AI changes our profession, I'm just along for the ride. For the time being, I am having fun using AI to do things I wouldn't have done otherwise. I recently built a <a href="https://github.com/nicolasbrailo/zmw/tree/main/zmw_cat_snack_dispenser">Cat feeder service</a> with Zigbee and Telegram integration. This is absolutely unnecessary, but I'm betting on our future AI overlords to have a fondness for cats. The training material makes me think AI will like cats more than humans. Can you blame it?</p>
<p><a href="https://github.com/nicolasbrailo/zmw/tree/main/zmw_cat_snack_dispenser"><img alt="" src="https://raw.githubusercontent.com/nicolasbrailo/zmw/main/zmw_cat_snack_dispenser/README_screenshot.png" /></a></p>
<p>Disclaimer: no AI has been used to write notes in this blog, this is still a manual efforrt and all of the mistakes here are carefully handcrafted by humans (a single human, actually).</p>

</div>

<div id="sitefooter">
   |
  <a href="/blog/history.html">Archive</a> |
  <a href="/blog/rss.xml">RSS</a>
</div>

<script src="/search.js"></script>
<script src="/codehighlight.js"></script>

</body>
</html>

<h1>Blog title<a name="blogtitle"></a></h1>
<h1>Menu<a name="menu"></a></h1>
<ul>
<li><a href="/">Home</a></li>
<li><a href="/blog/index.html">Blog</a></li>
<li><a href="/blog/history.html">History</a></li>
</ul>
<h1>Starting an EMR job with Boto<a name="startinganemrjobwithboto"></a></h1>
<p>I've noticed there are not many articles about boto and Amazon web services. Although boto's documentation is quite good, it lacks some practical examples. Most specifically, I found quite a fair amount of RTFM was needed to get an elastic map reduce job started on Amazon using Boto (and I did it from Google app engine, just to go full cloud!). So here it goes, a very basic EMR job launcher using boto:</p>
<pre lang="c++" style="display: inline-block; border: 1px solid red;">zone_name = &#x27;eu-west-1&#x27;
access_id = ...
private_key = ...
# Connect to EMR
conn = EmrConnection(access_id, private_key,
                    region=RegionInfo(name=zone_name,
                    endpoint= zone_name + &#x27;.elasticmapreduce.amazonaws.com&#x27;))
# Create a step for the EC2 instance to install Hive
args = [u&#x27;s3://&#x27;+zone_name+&#x27;.elasticmapreduce/libs/hive/hive-script&#x27;,
            u&#x27;--base-path&#x27;, u&#x27;s3://&#x27;+zone_name+&#x27;.elasticmapreduce/libs/hive/&#x27;,
            u&#x27;--install-hive&#x27;, u&#x27;--hive-versions&#x27;, u&#x27;0.7.1&#x27;]
start_jar = &#x27;s3://&#x27;+zone_name+ \
            &#x27;.elasticmapreduce/libs/script-runner/script-runner.jar&#x27;
setup_step = JarStep(&#x27;Hive setup&#x27;, start_jar, step_args=args)
# Create a jobflow using the connection to EMR and specifying the
# Hive setup step
jobid = conn.run_jobflow(
                    &quot;Hive job&quot;, log_bucket.get_bucket_url(),
                    steps=[setup_step],
                    keep_alive=keep_alive, action_on_failure=&#x27;CANCEL_AND_WAIT&#x27;,
                    master_instance_type=&#x27;m1.medium&#x27;,
                    slave_instance_type=&#x27;m1.medium&#x27;,
                    num_instances=2,
                    hadoop_version=&quot;0.20&quot;)
# Set the termination protection, so the job id won&#x27;t be killed after the
# script is finished (that way we can reuse the instance for something else
# Don&#x27;t forget to shut it down when you&#x27;re done!
conn.set_termination_protection(jobid, True)
s3_url = &#x27;Link to a Hive SQL file in S3&#x27;
args = [&#x27;s3://&#x27;+zone_name+&#x27;.elasticmapreduce/libs/hive/hive-script&#x27;,
        &#x27;--base-path&#x27;, &#x27;s3://&#x27;+zone_name+&#x27;.elasticmapreduce/libs/hive/&#x27;,
        &#x27;--hive-versions&#x27;, &#x27;0.7.1&#x27;,
        &#x27;--run-hive-script&#x27;, &#x27;--args&#x27;,
        &#x27;-f&#x27;, s3_url]
start_jar = &#x27;s3://&#x27;+zone_name+&#x27;.elasticmapreduce/libs/script-runner/script-runner.jar&#x27;
step = JarStep(&#x27;Run SQL&#x27;, start_jar, step_args=args)
conn.add_jobflow_steps(jobid, [step])
</pre>
<hr />
<p>Post by Nico Brailovsky @ 2013-07-11 -
    <a href="/blog/2013/0711_StartinganEMRjobwithBoto.html">Permalink</a> -
    <a href="https://github.com/nicolasbrailo/nicolasbrailo.github.io/issues/new?title=Comment@/blog_md/2013/0711_StartinganEMRjobwithBoto.md&amp;body=I%20have%20a%20comment!">Leave a comment</a></p>